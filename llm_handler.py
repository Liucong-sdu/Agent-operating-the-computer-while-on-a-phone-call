# llm_handler.py (å·²æ›´æ–°)
import json
import config
import openai
import threading


class LLMHandler:
    def __init__(self, client):
        self.client = client
        self._is_active = False
        self._lock = threading.Lock()

    def is_active(self):
        with self._lock:
            return self._is_active
            
    def _set_active(self, active):
        with self._lock:
            self._is_active = active

    def _is_complete_sentence(self, text):
        if text.strip().endswith(('.', '?', '!', 'ã€‚', 'ï¼Ÿ', 'ï¼', ':', 'ï¼š')):
            return True
        return False

    def get_llm_response_stream(self, history, tools=None): # <--- ã€æ”¹é€ ã€‘å…è®¸ tools ä¸º None
        """
        ä»å¤§æ¨¡å‹è·å–æµå¼å“åº”ï¼Œç¼“å†²æ–‡æœ¬ç›´åˆ°å½¢æˆå®Œæ•´å¥å­å†yieldã€‚
        """
        self._set_active(True)
        print("ğŸ§  LLMå¤„ç†å™¨: æ­£åœ¨è·å–å¤§æ¨¡å‹å“åº”æµ...")
        
        sentence_buffer = ""
        
        try:
            # ã€æ”¹é€ ã€‘æ ¹æ® tools æ˜¯å¦å­˜åœ¨ï¼Œæ„é€ ä¸åŒçš„è¯·æ±‚å‚æ•°
            request_params = {
                "model": config.LARGE_LLM_MODEL,
                "messages": history,
                "stream": True,
            }
            if tools:
                request_params["tools"] = tools
                request_params["tool_choice"] = "auto"

            stream = self.client.chat.completions.create(**request_params)

            tool_calls = []
            for chunk in stream:
                delta = chunk.choices[0].delta
                
                if delta and delta.content:
                    sentence_buffer += delta.content
                    
                    if self._is_complete_sentence(sentence_buffer):
                        print(f"ğŸ§  LLMå¤„ç†å™¨: äº§å‡ºä¸€ä¸ªå®Œæ•´å¥å­ -> '{sentence_buffer.strip()}'")
                        yield "text", sentence_buffer.strip()
                        sentence_buffer = ""

                if delta and delta.tool_calls:
                    for tool_call_chunk in delta.tool_calls:
                        if len(tool_calls) <= tool_call_chunk.index:
                            tool_calls.append({"id": "", "type": "function", "function": {"name": "", "arguments": ""}})
                        
                        tc = tool_calls[tool_call_chunk.index]
                        if tool_call_chunk.id:
                            tc["id"] = tool_call_chunk.id
                        if tool_call_chunk.function.name:
                            tc["function"]["name"] = tool_call_chunk.function.name
                        if tool_call_chunk.function.arguments:
                            tc["function"]["arguments"] += tool_call_chunk.function.arguments

            if sentence_buffer.strip():
                print(f"ğŸ§  LLMå¤„ç†å™¨: äº§å‡ºæµæœ«å°¾çš„å‰©ä½™æ–‡æœ¬ -> '{sentence_buffer.strip()}'")
                yield "text", sentence_buffer.strip()

            for tool_call in tool_calls:
                 if tool_call["id"] and tool_call["function"]["name"] and tool_call["function"]["arguments"]:
                    try:
                        json.loads(tool_call["function"]["arguments"])
                        yield "tool_call", openai.types.chat.chat_completion_message_tool_call.ChatCompletionMessageToolCall(**tool_call)
                    except json.JSONDecodeError:
                        print(f"ğŸ§  LLMå¤„ç†å™¨: è­¦å‘Š - å·¥å…·è°ƒç”¨çš„å‚æ•°ä¸æ˜¯ä¸€ä¸ªå®Œæ•´çš„JSON: {tool_call['function']['arguments']}")
                        continue

        except Exception as e:
            print(f"ğŸ§  LLMå¤„ç†å™¨: è·å–LLMå“åº”æ—¶å‡ºé”™: {e}")
        finally:
            self._set_active(False)
            print("ğŸ§  LLMå¤„ç†å™¨: å“åº”æµå¤„ç†å®Œæˆï¼ŒçŠ¶æ€å·²é‡ç½®")