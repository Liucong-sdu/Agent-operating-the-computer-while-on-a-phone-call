# audio_handler.py (æœ€ç»ˆä¼˜åŒ–ç‰ˆ - æ ¹æ²»çº¿ç¨‹é¥¿æ­»é—®é¢˜)

import pyaudio
import collections
import numpy as np
import onnxruntime as ort
import threading
import time
import os
import config

class AudioHandler:
    def __init__(self, on_speech_start, on_speech_end):
        self.on_speech_start = on_speech_start
        self.on_speech_end = on_speech_end
        self.stop_event = threading.Event()
        self.buffer_lock = threading.Lock() # æ–°å¢ï¼šç”¨äºä¿æŠ¤å…±äº«ç¼“å†²åŒºçš„é”

        self.vad_threshold = config.VAD_THRESHOLD
        self.min_speech_duration_samples = int(config.VAD_MIN_SPEECH_DURATION_MS / 1000 * config.AUDIO_SAMPLING_RATE)
        
        self.cooldown_until = 0
        self.cooldown_duration_s = 0.5 

        model_path = os.path.join(os.path.dirname(__file__), 'models', 'silero_vad.onnx')
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Silero VAD æ¨¡å‹æœªåœ¨ {model_path} æ‰¾åˆ°")
            
        self.vad_session = ort.InferenceSession(model_path)
        self._state = np.zeros((2, 1, 128), dtype=np.float32)
        self.sr_input = np.array([config.VAD_SAMPLING_RATE], dtype=np.int64)

        self.p = pyaudio.PyAudio()
        self.stream = self.p.open(
            format=pyaudio.paInt16,
            channels=config.AUDIO_CHANNELS,
            rate=config.AUDIO_SAMPLING_RATE,
            input=True,
            frames_per_buffer=config.AUDIO_CHUNK_SIZE
        )

        self.is_speaking = False
        self.speech_buffer = collections.deque()
        self.silence_chunks_after_speech = 0
        self.ring_buffer = collections.deque(maxlen=5)

        self.listen_thread = threading.Thread(target=self.listen_and_detect, daemon=True)

    def get_speech_buffer_snapshot(self):
        # çº¿ç¨‹å®‰å…¨åœ°è·å–ç¼“å†²åŒºçš„å¿«ç…§
        with self.buffer_lock:
            return b''.join(self.speech_buffer)

    def _process_chunk(self, chunk):
        audio_int16 = np.frombuffer(chunk, dtype=np.int16)
        audio_float32 = audio_int16.astype(np.float32) / 32768.0
        ort_inputs = {
            'input': np.expand_dims(audio_float32, axis=0),
            'sr': self.sr_input,
            'state': self._state
        }
        ort_outs = self.vad_session.run(None, ort_inputs)
        self._state = ort_outs[1]
        speech_prob = ort_outs[0].item()
        return speech_prob > self.vad_threshold

    def listen_and_detect(self):
        print("ğŸ¤ éŸ³é¢‘å¤„ç†å™¨: å¼€å§‹ç›‘å¬...")
        while not self.stop_event.is_set():
            try:
                chunk = self.stream.read(config.AUDIO_CHUNK_SIZE, exception_on_overflow=False)
                is_speech = self._process_chunk(chunk)

                if is_speech:
                    if not self.is_speaking and time.time() > self.cooldown_until:
                        print("ğŸ¤ éŸ³é¢‘å¤„ç†å™¨: æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹ã€‚")
                        self.is_speaking = True
                        # å¯åŠ¨ä¸€ä¸ªæ–°çº¿ç¨‹æ¥å¤„ç† on_speech_startï¼Œé¿å…é˜»å¡ VAD å¾ªç¯
                        threading.Thread(target=self.on_speech_start).start()
                        
                        # åœ¨é”ä¿æŠ¤ä¸‹å°† ring_buffer çš„å†…å®¹è½¬ç§»åˆ° speech_buffer
                        with self.buffer_lock:
                            for prev_chunk in self.ring_buffer:
                                self.speech_buffer.append(prev_chunk)
                            self.ring_buffer.clear()
                    
                    if self.is_speaking:
                        # åœ¨é”ä¿æŠ¤ä¸‹å‘ speech_buffer æ·»åŠ æ•°æ®
                        with self.buffer_lock:
                            self.speech_buffer.append(chunk)
                        self.silence_chunks_after_speech = 0
                else:
                    if self.is_speaking:
                        with self.buffer_lock:
                            self.speech_buffer.append(chunk)
                        self.silence_chunks_after_speech += 1
                        
                        silence_duration_ms = self.silence_chunks_after_speech * config.VAD_FRAME_MS
                        if silence_duration_ms >= config.VAD_MAX_SILENCE_DURATION_MS:
                            print("ğŸ¤ éŸ³é¢‘å¤„ç†å™¨: æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸã€‚")
                            self.cooldown_until = time.time() + self.cooldown_duration_s
                            
                            with self.buffer_lock:
                                final_audio = b''.join(list(self.speech_buffer))
                                self.speech_buffer.clear()

                            self.is_speaking = False
                            self.ring_buffer.clear()
                            self.silence_chunks_after_speech = 0
                            
                            if len(final_audio) >= self.min_speech_duration_samples * 2:
                                threading.Thread(target=self.on_speech_end, args=(final_audio,)).start()
                    else:
                        self.ring_buffer.append(chunk)

            except Exception as e:
                print(f"ğŸ¤ éŸ³é¢‘å¤„ç†å™¨: ç›‘å¬å¾ªç¯å‡ºé”™: {e}")
                self.is_speaking = False
                with self.buffer_lock:
                    self.speech_buffer.clear()
                self.ring_buffer.clear()
            
            # ã€ç»ˆæä¿®å¤ã€‘å¼ºåˆ¶çº¿ç¨‹ä¼‘æ¯0.01ç§’ï¼Œå°†CPUæ—¶é—´ï¼ˆå’ŒGILï¼‰è®©ç»™å…¶ä»–çº¿ç¨‹
            time.sleep(0.01)

    def start(self):
        self.listen_thread.start()

    def stop(self):
        self.stop_event.set()
        print("ğŸ¤ éŸ³é¢‘å¤„ç†å™¨: æ­£åœ¨åœæ­¢...")
        if self.listen_thread.is_alive():
            self.listen_thread.join(timeout=2)
        if self.stream.is_active():
            self.stream.stop_stream()
        self.stream.close()
        self.p.terminate()
        print("ğŸ¤ éŸ³é¢‘å¤„ç†å™¨: å·²åœæ­¢ã€‚")